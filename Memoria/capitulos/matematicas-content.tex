
\newpage
\thispagestyle{empty}
\vspace*{\fill}
\begin{center}
    \large Parte II \\
    \vspace{0.5cm}           
    \LARGE \textbf{FUNDAMENTOS MATEMÁTICOS}
\end{center}
\vspace*{\fill}
\newpage
\setcounter{page}{1}  

\newpage

\chapter{Técnicas multivariantes: fundamentos y desarrollo del análisis clúster}

El análisis de datos en ciencias ómicas requiere metodologías capaces de manejar la complejidad 
inherente a los sistemas estudiados. En particular, las técnicas multivariantes han demostrado ser 
herramientas fundamentales para la exploración, modelado e interpretación de datos de alta dimensión. 
Estas metodologías permiten identificar relaciones entre variables, reducir la dimensionalidad y 
clasificar observaciones en función de patrones subyacentes. \newline

Este capítulo está estructurado en dos secciones. En primer lugar, se presentarán las principales 
técnicas multivariantes, destacando su utilidad y objetivos dentro del análisis de datos. Posteriormente, 
se abordará en profundidad el análisis clúster, una técnica multivariante ampliamente utilizada para 
la identificación de patrones en grandes volúmenes de datos. Su aplicación en el ámbito biológico 
permite revelar estructuras subyacentes en datos complejos, facilitando la comprensión de procesos 
como la agrupación de expresiones génicas o la clasificación de organismos en función de sus características. \newline

Dado que en el capítulo dedicado a los datos ómicos hemos introducido la matriz de datos ómicos $X$, que 
representa las $N$ características medidas sobre $n$ muestras, mantendremos esta notación en el desarrollo de
los fundamentos matemáticos sobre los que se basa este trabajo. Así, consideraremos que la matriz de expresión 
$X \in \mathbb{R}^{N \times n}$ almacena las observaciones de nuestras variables, con filas representando las
características y columnas las muestras. 



\section{Preliminares}

La información aquí recogida se ha extraído principalmente de las fuentes \cite{hist-mul-1}, \cite{hist-mul-2}, \cite{Bib-1}. \newline %history-mul-1, history-mul-2, %Bib-1

El análisis multivariante es una herramienta clave para explorar y comprender la complejidad de los sistemas 
biológicos, económicos y sociales. Su capacidad para procesar múltiples variables simultáneamente permite identificar 
patrones ocultos en grandes volúmenes de datos. \newline

Las técnicas multivariantes son fundamentales para abordar la complejidad de los datos en diversas disciplinas, 
incluyendo las ciencias ómicas, donde se requieren metodologías capaces de gestionar la alta dimensionalidad y 
variabilidad de los datos obtenidos. Estas herramientas permiten descubrir relaciones entre variables, 
reducir la dimensionalidad y clasificar observaciones, lo que facilita la interpretación y el modelado de sistemas 
complejos. \newline

El desarrollo del análisis multivariante se remonta a principios del siglo XX, cuando pioneros como Karl Pearson y R.A
Fisher introdujeron técnicas fundamentales como el análisis de componentes principales y el análisis discriminante.
Posteriormente, C.R. Rao y otros investigadores expandieron estos métodos, estableciendo bases matemáticas sólidas que
han permitido su aplicación en un amplio espectro de disciplinas. Estas técnicas han ido desarrollándose exponencialmente
con el avance de la computación, facilitando el procesamiento de grandes volúmenes de datos y dando lugar a análisis 
mucho más sofisticados en muchas áreas como la biología, la economía, las ciencias sociales, etc. \newline

En términos generales, las metodologías multivariantes pueden dividirse en dos grandes enfoques: \textit{descriptivo}
e \textit{inferencial}. El primero busca simplificar la estructura de los datos y revelar relaciones latentes entre
variables, mientras que el segundo permite realizar pruebas de hipótesis considerando múltiples variables de manera
simultánea, asegurando la validez estadística de los resultados. La elección de la técnica adecuada depende del tipo 
de datos y de la pregunta de investigación. A continuación, se presentan algunas de las principales metodologías multivariantes:

\subsection{Análisis de Componentes Principales (PCA)}

El \textit{análisis de componentes principales (PCA)} fue introducido por primera vez por Karl Pearson a principios del siglo XX. El 
tratamiento formal de esta técnica se debe a Hotelling (1933) y Rao (1964). Su propósito era facilitar la comprensión de conjuntos 
de datos complejos mediante la reducción de su dimensionalidad, minimizando la pérdida de información. En PCA, un conjunto de $N$ variables correlacionadas
se transforma en un conjunto más pequeño de constructos hipotéticos no correlacionados llamados \textit{componentes principales}.
Su objetivo es condensar la información proporcionada por dichas variables en unas pocas de ellas o en pocas combinaciones lineales de 
ellas (con máxima variabilidad). \newline %referencia PCA romero béjar, % referencia: Bib-5


Las componentes principales se definen como combinaciones lineales de las variables originales que capturan la mayor variabilidad
posible en los datos. Matemáticamente, si $Y$ es un vector de $N$ variables observadas con media $\mu$ y matriz de covarianza $\Sigma$,
las componentes principales $Z_{i}$ se obtienen como:

%duda: ¿tengo que explicar que suponemos normalidad multivariante?

\[
Z_{i} = p^{'}_{i} Y, \hspace{0.2cm} i=1,2,...,N
\]

donde $p_{i}$ es un vector de pesos o \textit{cargas principales} que maximizan la varianza de $Z_{i}$ bajo la restricción de que $p_{i}$
tiene norma unitaria, es decir,

\[%duda: ¿tengo que decir la norma que estoy usando?
\max Var(Z_{i}) = p_{i}^{'}\Sigma p_{i}, \text{ sujeto a } p_{i}^{'}p_{i} = 1.
\]

y tal que asegura que las componentes principales son ortogonales entre sí, es decir:

\[
p_{i}^{'}p_{j} = 0, \text{ para } i \neq j.
\]

Así, garantizamos que las componentes principales $Z_{i}$ y $Z_{j}$ son intercorrelacionadas, es decir, su covarianza es cero para $i\neq j$.


Los vectores $p_{i}$ son los autovectors de la matriz de covarianza $\Sigma$, y los valores propios $\lambda_{i}$, corresponden
a la varianza explicada por cada componente principal. La transformación completa de los datos se expresa de la siguiente forma:

\[
Z=P^{'}Y
\]

donde $P$ es la matriz de autovectores de $\Sigma$, lo que garantiza que las componentes principales sean ortogonales entre sí y no 
correlacionadas, cada una con las anteriores. \newline

Las CP se utilizan para descubrir e interpretar las dependencias que existen entre las variables y para examinar las relaciones
que pueden existir entre los individuos. También son útiles para estabilizar las estimaciones, evaluar la normalidad
multivariante y detectar valores atípicos. \newline

% referencias: bib-5, biib-6, PCA-béjar

\subsection{Análisis factorial}

El objetivo principal del \textit{análisis factorial} (AF) es capturar la realidad de la manera más simple posible, identificando 
unas pocas variables latentes\footnote[8]{Variable latente: variable no observable que se infiere a partir de un conjunto de variables 
observables utilizando un modelo matemático. } que definen esa realidad. Esta técnica multivariante busca explicar el comportamiento de las $N$ 
variables en la matriz de datos $X$ utilizando un número reducido de variables latentes, denominadas \textit{factores}. Lo ideal es que toda 
la información contenida en $X$ pueda ser representada mediante un número menor de factores. Esta técnica busca explicar las correlaciones 
entre las variables mediante la combinación lineal de dichos factores. Así, cada factor es una variable latente que 
influye en las variables observadas, y cuya presencia se infiere a partir de las correlaciones entre ellas. \newline

Matemáticamente, cada variable observada, $x\in \mathbb{R}^{N}$, 
se expresa como una combinación lineal de estos factores, más un término de error específico:

\[
x_{i} = \sum_{l=1}^{k}q_{il}f_{l} + \mu_{i}, \text{ } i=1,...,N.
\]

Aquí, $f_{l}$, con $l=1,...,k$ denota a los factores. El número de factores, $k$, debería ser siempre mucho más pequeño que $N$. \newline

En definitiva, el modelo de análisis factorial asume que la variable observada $x$ puede descomponerse en dos componentes: una parte explicada
por los factores comunes y una parte específica de cada variable. Esto se expresa matricialmente de la siguiente forma:

\[
x=\Lambda F + \psi
\]

donde:

\begin{itemize}
    \item $\Lambda$ es la matriz de cargas factoriales de dimensión $N \times k$,
    \item $F$ es el vector de factores de dimensión $k$.
    \item $\psi$ es el vector de factores específicos o residuales de dimensión $N$
\end{itemize}

El análisis factorial fue desarrollado por Charles Spearman a principios del siglo XX para modelar la inteligencia humana, 
postulando que las puntuaciones en distintas pruebas estaban intercorrelacionadas debido a un único factor latente de inteligencia 
general (g). Su modelo de un solo factor fue posteriormente generalizado por Thurstone a múltiples factores. \newline

El análisis de componentes principales (PCA)  y el análisis factorial suelen confundirse porque ambos analizan 
la variación en un conjunto de variables a partir de la matriz de correlación o covarianza. Sin embargo, mientras que en el EFA unas 
pocas variables latentes explican las correlaciones observadas, en el PCA se necesitan todos los componentes principales para describir 
completamente la variabilidad. Así, el PCA se centra en explicar la varianza total, mientras que el EFA se enfoca en las relaciones 
entre las variables mediante factores comunes. 

%refernecia: Bib-5, Bib-6, AF-romero bejar.


\subsection{Análisis Discriminante}

El \textit{análisis discriminante} es una técnica multivariante exploratoria que permite identificar un subconjunto de variables y funciones
asociadas que maximicen la separación entre los grupos o poblaciones de estudio. Su objetivo principal es construir funciones
discriminantes que describan y caractericen la separación de los grupos, evaluar el grado de diferenciación y analizar la contribución
de cada variable a la discriminación. \newline

Cuando estas funciones son combinaciones lineales de las variables originales, se denominan funciones discriminantes lineales (LDF). En
particular, el análisis discriminante lineal de Fisher busca encontrar una combinación lineal de variables que maximice la separación
entre grupos. Para dos grupos con medias $\mu_{1}$ y $\mu_{2}$ y una matriiz de covarianza común $\Sigma$, la función discriminante de Fisher
se define como:

\[
L = a^{'}y = \sum_{j=1}^{N} a_{j}y_{j}
\]

donde $a$ es el vector de coeficientes de la función discriminante e $y$ es el vector de observaciones de las variables. \newline

El vector $a$ que maximiza la separación entre los grupos se obtiene como:

\[
a_{\Sigma} = \Sigma^{-1}(\mu_{1}-\mu_{2})
\]

Además, la \textit{distancia de Mahalanobis}, que explicaremos con detalle en la siguiente sección, se emplea para medir la separación entre
los centroides de los grupos:

\[
D^{2} = (\mu_{1}-\mu_{2})^{'}S^{-1}(\mu_{1}-\mu_{2})
\]

Si $D^{2}$ es significativo, implica una buena discriminación entre los grupos. \newline

Este análisis tiene aplicaciones en diversos campos: en biología, Fisher (1936) lo utilizó para diferenciar especies de iris en función de
características morfológicas; en la gestión de personal, permite clasificar profesionales según sus habilidades; en medicina, ayuda a distinguir
entre individuos con alto o bajo riesgo de enfermedades; y en la industria, contribuye a identificar cuándo un proceso está bajo control o fuera 
de control. \newline

Para el caso de múltiples grupos, la función discriminante se construye de manera que maximice la variabilidad entre los grupos en relación con 
la variabilidad dentro de los grupos, lo que se logra mediante una descomposición en valores propios. \newline

Toda la información ha sido extraída de las fuentes bibliográficas\cite{bejar-PCA},\cite{bejar-AF}\cite{Bib-5},\cite{Bib-6} %bib-5, biib-6, PCA-béjar, AF-romero bejar

\section{Análisis Clúster}
